\chapter{\label{ch3}Aprendizaje Automático}

De acuerdo con Tom Mitchell, el Aprendizaje Automático consiste en el estudio de algoritmos que mejoran automáticamente a través de la experiencia. 

\section{El problema del aprendizaje}

Cuando decimos que un algoritmo aprende, ¿qué entendemos por aprender? 

\begin{definition*}
	Se dice que un algoritmo o programa $\mathcal{A}$ aprende de la experiencia $E$ respecto a alguna tarea $T$ y una medida de rendimiento $P$, si su rendimiento en la tarea $T$ de acuerdo a $P$ mejora con la experiencia $E$.
\end{definition*}

Podemos formular el problema del aprendizaje en los siguientes términos. Sean $\mathcal{X, Y}$ dos conjuntos que llamaremos espacio de entrada y de salida. Sea $\mathcal{D}$ el conjunto de datos o ejemplos.

Queremos aproximar una función objetivo $f: \mathcal{X} \mapsto \mathcal{Y}$ de acuerdo a una medida de error $L: \mathcal{H} \mapsto \mathbb{R}$. Nuestro algoritmo $\mathcal{A}$ usará $\mathcal{D}$ para elegir una función $g: \mathcal{X} \mapsto \mathcal{Y}$ de un conjunto de hipótesis $\mathcal{H}$ que aproxime a $f$.

La tarea $T$ se corresponde con encontrar la función $f$, la experiencia $E$ con el conjunto de datos $\mathcal{D}$, y la medida de rendimiento con la medida de error $L$.

El problema al que nos enfrentamos en este trabajo es un \textbf{problema de clasificación supervisado}. Veamos a qué nos referimos con esto. 

\subsection{Tipos de aprendizaje}
En función del conjunto de datos $\mathcal{D}$ del que aprende nuestro algoritmo encontramos dos grandes clases de aprendizaje:
\begin{itemize}
	\item \textbf{Aprendizaje supervisado:} cada ejemplo está etiquetado con el resultado que el algoritmo debería de producir. Es decir, $\mathcal{D}\subset\mathcal{X}\times\mathcal{Y}$ y se busca encontrar una relación entre la etiqueta y el ejemplo.
	\item \textbf{Aprendizaje no supervisado:} los datos no están etiquetados. Ahora $\mathcal{D}\subset\mathcal{X}$ y pretendemos identificar propiedades de la estructura del conjunto de datos. 
\end{itemize}

\subsection{Problema de clasificación}
El aprendizaje automático puede aplicarse para resolver multitud de problemas: clasificación, regresión, predicción de series temporales, detección de anomalías, generación de nuevos datos, etc.

En un problema de clasificación el espacio de salida consiste en una serie de categorías o clases y queremos que nuestro algoritmo asigne a cada ejemplo una clase de forma correcta. Es decir, $\mathcal{Y} = \{ 1, \ldots, C\}$ con $C \in \mathbb{N}$. Si $C=2$, tenemos un problema de clasificación binaria. 

De ahora en adelante, nos centraremos en el aprendizaje supervisado. Concretamente, hablaremos de problemas de clasificación.  

\section{Factibilidad del aprendizaje}
El algoritmo de aprendizaje usa el conjunto de entrenamiento $\mathcal{D}$, que en la práctica será un conjunto finito de ejemplos, para aprender la función objetivo. ¿Por qué un conjunto limitado de datos puede revelar información suficiente para lograr nuestro objetivo? Para responder a esta pregunta tendremos que añadir algunas hipótesis adicionales a la formulación del problema. Escribamos $\mathcal{D}=\{ (x_1, y_1), \ldots, (x_N, y_N) \}$ y supongamos:\\

\noindent
\textbf{H1.} Tras fijar el espacio $\mathcal{H}$, los ejemplos $x_1, \ldots, x_N$ han sido extraídos de forma independiente de acuerdo a una distribución de probabilidad $P$ sobre el espacio de entrada $\mathcal{X}$.\\ 

La función objetivo $f$ es completamente desconocida. Lo que obtenemos como resultado del aprendizaje es una función $g$ que aproxima a $f$. Para medir cómo de buena es esta aproximación, recurrimos a la función de pérdida $\ell$.
En la situación más general, tendremos una función $\ell: \mathcal{X} \times \mathcal{X} \times \mathcal{Y} \mapsto \mathbb{R}$, tal que $\ell(h, x, y)$ mide el error que cometemos al asignar a un ejemplo $x$ la etiqueta $h(x)$ cuando su etiqueta correcta es $y$. Definimos 
\begin{gather*}
	L(h) = \mathbb{E}_{X \sim P}[\ell(h, X, f(X))] \quad \forall h \in \mathcal{H}
\end{gather*}
y queremos resolver:
\begin{gather*}
	\text{Encontrar $g \in \mathcal{H}$ tal que } L(g) = \min_{h \in \mathcal{H}} L(h)
\end{gather*}
No tenemos acceso a la función $f$ ni tampoco a la distribución $P$. Por tanto, resolver el anterior problema será inviable en la mayor parte de las situaciones. Una forma natural de proceder es definir 
\begin{gather*}
	L_{in}(h, \mathcal{D}) =  \dfrac{1}{N}\sum_{(x_n, y_n) \in \mathcal{D}} \ell(h, x_n, y_n) \quad  \forall h \in \mathcal{H}
\end{gather*}
y, una vez fijado $\mathcal{D}$, minimizar $L_{in}$ en lugar de $L$:
\begin{gather*}
	\text{Encontrar $g \in \mathcal{H}$ tal que } L_{in}(g) = \min_{h \in \mathcal{H}} L_{in}(h)
\end{gather*}
Lo que vamos a probar es que reducir $L_{in}$ conlleva reducir $L$, luego podemos aspirar a soluciones subóptimas.

Para un problema de clasificación binaria, definimos $\ell(u,v)=1$ si $u=v$ y $0$ si $u\neq v$. Vamos a omitir la dependencia de $\mathcal{D}$ al escribir $L_{in}$ y vamos a llamar $L_{out} = L(h) = P[h(x) \neq f(x)] \quad \forall h \in \mathcal{H}$. Vamos a usar la desigualdad de Hoeffding:

\begin{lemma}[Desigualdad de Hoeffding]\label{hoeffding}
	Sea $Z_1, \ldots, Z_N$ una muestra aleatoria simple de una variable aleatoria $Z$ que sigue una distribución de Bernouilli con media $E[Z]$. Sea $\bar{Z}$ la media muestral. Sea $\epsilon$ un número real estrictamente positivo. Entonces se cumple que:
	\begin{align}
		P[ |E[Z] - \bar{Z}| > \epsilon  ] \leq 2 \epsilon^{-2 \epsilon^2 N}
	\end{align}
\end{lemma}

Supongamos primero que $\mathcal{H} = \{ h \}$. Definamos $I: \mathcal{X} \mapsto \{0,1\}$ como $I(x)=1$ si $h(x) = f(x)$ y $I(x) = 0$ si $h(x) \neq f(x)$. Tenemos entonces que $I$ es una variable aleatoria que sigue una distribución de Bernouilli, cuya esperanza se corresponde con $L_{out}$. Si escribimos $I_n=\ell(x_n, y_n)$, tenemos que $I_1, \ldots, I_n$ es una muestra aleatoria simple de $I$, y podemos aplicar (\ref{hoeffding}) para un $\epsilon > 0$ fijo obteniendo que:
\begin{align}\label{desig-h}
	P[ |L_{out}(h) - L_{in}(h)| > \epsilon  ] \leq 2 \epsilon^{-2 \epsilon^2 N}
\end{align}

Esta desigualdad nos dice que, fijado un margen de error $\epsilon$, la diferencia entre $L_{in}$ y $L_{out}$ queda controlada por el tamaño de la muestra. Aumentando el tamaño de $N$, la probabilidad de exceder el margen de error tiende a cero. 

Ahora supongamos que $\mathcal{H} = \{h_1, \ldots, h_M\}$. Denotemos por $g$ la solución aproximada que obtiene el algoritmo $\mathcal{A}$. Denotemos por $\mathbb{B}$ el evento $|L_{out}(g) - L_{in}(g)| > \epsilon$ y para cada $h_m$, por $\mathbb{B}_m$ el evento $|L_{out}(h_m) - L_{in}(h_m)| > \epsilon$. Se cumple que $ \mathbb{B} \subset \bigcup_{m=1}^{M} \mathbb{B}_m$. Entonces:
\begin{align*}
	P[\mathbb{B}] \leq P[\bigcup_{m=1}^{M} \mathbb{B}_m ] \leq \sum_{m=1}^{M} P[ \mathbb{B}_m ]
\end{align*}
Podemos aplicar la desigualdad (\ref{desig-h}) a cada término para obtener:
\begin{align*}
	P[ |L_{out}(g) - L_{in}(g)| > \epsilon ] \leq 2 M \epsilon^{-2 \epsilon^2 N}
\end{align*}
Hemos obtenido una generalización de la desigualdad (\ref{desig-h}) para el caso en que $\mathcal{H}$ tiene más de un elemento, y a la que se le puede aplicar un razonamiento análogo al que hacíamos antes. 

Vamos a escribir la desigualdad anterior de otra forma. Llamemos $\delta = 2 M \epsilon^{-2 \epsilon^2 N}$, luego $\epsilon = \sqrt{ \dfrac{1}{2N} log \left( \dfrac{2M}{\delta} \right) }$. Tenemos que, con probabilidad al menos $1-\delta$, se cumple:
\begin{align*}
	L_{out}(g) - L_{in}(g) \leq |L_{out}(g) - L_{in}(g)| \leq \epsilon
\end{align*}
de donde, nuevamente con probabilidad al menos $1-\delta$,
\begin{align}\label{desig_gen_1}
	L_{out}(g) \leq L_{in}(g) + \sqrt{ \dfrac{1}{2N} log \left( \dfrac{2M}{\delta} \right) }
\end{align}
Como vemos, fijada una tolerancia $\delta$ el error $L_{out}$ queda dominado por el error dentro de la muestra, $L_{in}$, más un término positivo. Este término positivo converge a cero cuando el tamaño de la muestra aumenta.


\subsection{Dimensión de Vapnik-Chervonenkis}
En la práctica, la clase de funciones entre la que buscamos la solución será infinita. Para dar una respuesta en este caso, debemos recurrir a un razonamiento más técnico. Exponemos aquí brevemente las ideas básicas de la teoría de la dimensión de Vapnik-Chervonenkis. 

Si $x_1, \ldots, x_N$ son $N$ puntos de $\mathcal{X}$, cada $h \in \mathcal{H}$ genera una dicotomía que podemos denotar por $(h(x_1), \ldots, h(x_N))$.
\begin{definition}[Dicotomías generadas por $\mathcal{H}$]
	Sean $x_1, \ldots, x_N \in \mathcal{X}$, las dicotomías generadas por $\mathcal{H}$ en $x_1, \ldots, x_N$ se denotan por $\mathcal{H}(x_1, \ldots, x_N) = \{ (h(x_1), \ldots, h(x_N)) \colon h \in \mathcal{H} \} $
\end{definition}

\begin{definition}[Función de crecimiento]
	Sea $\mathcal{H}$ una clase de hipótesis. Definimos la función de crecimiento de $\mathcal{H}$ como 
	\begin{align*}
		m_{\mathcal{H}} \colon \mathbb{N} &\to \mathbb{R}\\
		N &\mapsto \max_{x_1, \ldots, x_N \in \mathcal{X}}  \mathcal{H}(x_1, \ldots, x_N)
	\end{align*}
	donde $|A|$ denota el cardinal de $A$.
\end{definition}

Como  $\mathcal{H}(x_1, \ldots, x_N) \subset \{ 0, 1 \}^N$, entonces $m_{\mathcal{H}}(N) \leq 2^N$ para cada $N \in \mathbb{N}$. Si para algún natural $k$ ocurre que $m_{\mathcal{H}}(k) \leq 2^k$, podemos acotar la función de crecimiento por un polinomio de grado $k-1$:

\begin{prop}
	Si $m_{\mathcal{H}}(k) < 2^k$ para algún número natural $k$, entonces 
	\begin{align*}
		m_{\mathcal{H}}(N) \leq \sum_{i=0}^{k-1} \binom{N}{i} \quad \forall N \in \mathbb{N}
	\end{align*}
\end{prop}

%\begin{definition}[Punto de ruptura]
%	Sea $k \in \mathbb{N}$. Si $m_{\mathcal{H}}(k) < 2^k$, entonces $k$ es un punto de ruptura.
%\end{definition}

La anterior propiedad motiva el concepto clave de esta teoría. La dimensión de Vapnik-Chervonenkis nos permite medir cómo de grande es el conjunto de hipótesis.

\begin{definition}[Dimensión de Vapnik-Chervonenkis]
	Sea $\mathcal{H}$ una clase de hipótesis. La dimensión de Vapnik-Chervonenkis o dimensión VC de $\mathcal{H}$, denotada por $d_{VC}(\mathcal{H})$, es:
	\begin{align*}
		d_{VC}(\mathcal{H}) = \max \{ N \in \mathbb{N} \colon m_{\mathcal{H}}(N) = 2^N \}
	\end{align*}
	entendiendo que si $m_{\mathcal{H}}(N) = 2^N$ para cada $N \in \mathbb{N}$, entonces $d_{VC}(\mathcal{H}) = \infty$. 
\end{definition}

El resultado central de esta teoría, y el cual nos permitirá dar una respuesta a la pregunta que nos planteamos en esta sección, se enuncia como sigue:

\begin{theorem}[Cota VC]
	Sea $\delta >0$. Entonces:
	\begin{align*}
		L_{out}(g) \leq L_{in}(g) + \sqrt{ \dfrac{8}{N} log \left( \dfrac{4 m_{\mathcal{H}}(2N) }{\delta} \right) }
	\end{align*}
	con probabilidad al menos $1-\delta$.
\end{theorem}

Si $d_VC(\mathcal{H})$ es finita, entonces podemos aplicar la proposición 3.4. Al tomar límite cuando $N$ tiende a infinito, vemos que la diferencia entre $L_{out}$ y $L_{in}$ tiende a cero.

Como vemos, esta desigualdad guarda cierto parecido a la que obteníamos cuando $\mathcal{H}$ es finita. Al igual que en aquella, si podemos asegurar que el tamaño de la clase de hipótesis es finito (en este caso a través de la dimensión VC), podemos asegurar que la diferencia entre el error fuera y dentro de la muestra tiende a cero cuando aumenta el tamaño de esta.

\section{Optimización: gradiente descendente}
En la anterior sección hemos argumentado por qué para obtener un modelo de aprendizaje automático el problema que se resuelve es:
\begin{gather*}
	\text{Encontrar $g \in \mathcal{H}$ tal que } L_{in}(g) = \min_{h \in \mathcal{H}} L_{in}(h)
\end{gather*}
Por tanto, todos los esfuerzos se centran en minimizar una función $L_{in}$. En la práctica, cada hipótesis $h$ queda completamente determinada por unos parámetros $w \in \mathbb{R}^d$. Entonces, podemos reformular el problema de la siguiente forma:
\begin{gather*}
	\text{Encontrar $w^* \in \mathbb{R}^d$ tal que } L_{in}(w^*) = \min_{w \in \mathbb{R}^d} L_{in}(w)
\end{gather*}
Nos encontramos ante un problema de optimización y nos interesa conocer métodos para resolverlo.

Vamos a estudiar uno de los métodos más efectivos. En la práctica, los algoritmos basados en el gradiente descendente obtienen muy buenos resultados. Suponiendo que la función que queremos optimizar es diferenciable, la idea del gradiente descendente consiste en partir de un punto aleatorio, y, fijándonos en el gradiente, movernos en la dirección en la que decrece la función. 

\begin{algorithm}
	\KwIn{Función $F$, tasa de aprendizaje $\eta$}
	\SetAlgoLined
	Inicializar w(0)\\
	\For{t = 0, 1, 2, $\ldots$}{
		$w(t+1) = w(t) - \eta \nabla F(w(t))$
	}
	\caption{Gradiente descendente}
\end{algorithm}


\subsection{Convergencia del método}
Podemos probar que, bajo ciertas hipótesis no muy restrictivas, el algoritmo llega a obtener un mínimo global. Para ello, vamos a demostrar algunos resultados previos. Fijemos una función $F \colon \mathbb{R}^d \mapsto \mathbb{R}$ tal que $F \in \mathcal{C}^1$. Denotemos por $\| \cdot \|$ a la norma euclídea.

\begin{definition}[Función convexa]
	Decimos que $F$ es convexa si
	\begin{align*}
		F(tx + (1-t)y) \leq tF(x) + (1-t)F(y) \quad \forall x, y \in \mathbb{R}^d, t \in [0,1]
	\end{align*}
\end{definition}

\begin{definition}[Gradiente Lipschitz]
	Decimos que el gradiente de $F$ es lipschitziano con constante $L>0$ si 
	\begin{align*}
		\| \nabla F(x) - \nabla F(y)\| \leq L \| x-y \| \quad \forall x, y \in \mathbb{R}^d
	\end{align*}
\end{definition}

\begin{lemma}
	\leavevmode
	Sean $x, y \in \mathbb{R}^d$.
	\begin{enumerate}
		\item Si $F$ es convexa, cualquier mínimo local es un mínimo global. 
		\item Si $F$ es convexa, entonces 
		\begin{align}\label{desig-convex}
			\nabla F(y)^T (x-y) \leq F(x) - F(y)
		\end{align}
		\item Si el gradiente de $F$ es lipschitziano con constante $L>0$, entonces 
		\begin{align}\label{desig-lips}
			F(y) \leq F(x) + \nabla F(x)^T(y-x) + \dfrac{L}{2} \| y-x\|^2
		\end{align}
	\end{enumerate}
\end{lemma}
\begin{proof}
	\leavevmode
	\begin{enumerate}
		\item Supongamos que $x^*$ es un mínimo local. Entonces existe $r>0$ tal que $F(x^*) < F(x)$ para cada $x \in B(x^*, r)$. Supongamos para llegar a contradicción que existe $z$ tal que $F(z)<F(x^*)$. Entonces, para $t \in [0,1]$:
		$$ F( tx^* + (1-t)z  ) \leq tF(x^*) + (1-t)F(z) < F(x^*) $$
		Evaluando en $t=1$ se obtiene que $F(x^*)<F(x^*)$, lo cual no es posible.
		\item Fijemos $x, y \in \mathbb{R}^d$. De la definición de convexidad, se cumple que:
		$$F(y + t(x-y)) - F(y) \leq t (F(x) - F(y)) $$
		Diviendo por $t>0$ ambos términos, podemos tomar límite cuando $t$ tiende a cero y aplicar la definición de diferencial obteniendo la desigualdad buscada.
		\item Definimos la función $g(t) = f(x + t(y-x))$ para cada $t \in [0,1]$. Se cumple que $g$ es derivable con $g'(t) = \nabla F(x + t(y-x))^T(y-x)$ para cada $t \in [0,1]$. Aplicando la desigualdad de Cauchy-Schwartz y la definición de gradiente lipschiztiano:
		\begin{align*}
			g'(t) - g'(0) & = \left[ \nabla F(x + t(y-x)) - \nabla F(x) \right]^T (y-x) \leq \\
			&\leq  \| \nabla F(x + t(y-x)) - \nabla F(x) \| \| y-x \| \leq L t \| y-x \|^2
	 	\end{align*}
 		Por el teorema fundamental del cálculo:
 		\begin{align*}
 			F(y) & = g(1) = g(0) + \int_{0}^{1}g'(t)dt \leq g(0) + g'(0) + L \| y-x \|^2 \int_{0}^{1}t dt= \\
 			& = F(x) + \nabla F(x)^T(y-x) + \dfrac{L}{2} \| y-x\|^2
 		\end{align*}
	\end{enumerate}
\end{proof}

Podemos probar ya la convergencia del método del gradiente descendente. Supondremos la existencia de un punto donde se alcanza el mínimo de la función $F$. 
\begin{prop} \label{prop:convergencia_grad_desc}
	Sea $F \colon \mathbb{R}^d \mapsto \mathbb{R}$ tal que $F \in \mathcal{C}^1$, es convexa y su gradiente es lipschitziano con constante $L>0$. Supongamos que existe $x^*$ tal que $F(x^*) \leq F(x)$ para todo $x \in \mathbb{R}^d$. Sea $\eta \leq \dfrac{1}{L}$. Sea $x_0 \in \mathbb{R}^d$ y definamos $x_k = x_{k-1} - \eta \nabla F(x_{k-1})$ para cada $k \in \mathbb{N}$. Entonces:
	\begin{align*}
		F(x_k) - F(x^*) \leq \dfrac{\| x_0 - x^* \|^2}{2 \eta k}
	\end{align*} 
\end{prop}

\begin{proof}
	Vamos a fijar $x \in \mathbb{R}^d$ y llamamos $y = x - \eta \nabla F(x)$. De la desigualdad (\ref{desig-lips}) tenemos que:
	$$ F(y) \leq F(x) - \eta \| \nabla F(x) \|^2 + \dfrac{1}{2} L \eta^2 \| \nabla F(x) \|^2 $$
	Usando que $\eta \leq \dfrac{1}{L}$, llegamos a:
	$$ F(y) \leq F(x) - \dfrac{1}{2} \eta \| \nabla F(x) \|^2 $$
	En particular, $F(y) \leq F(x)$, y por tanto $F(x_k) \leq F(x_i)$ para cada $i \leq k$, lo cual prueba que la actualización del gradiente descendente conlleva el decrecimiento de la función objetivo en cada iteración. Ahora usando la desigualdad (\ref{desig-convex}), obtenemos
	$$ F(x) \leq F(x^*) + \nabla F(x)^T (x-x^*)$$
	Combinando las dos últimas desigualdades:
	\begin{gather*}
		F(y)  \leq F(x^*) + \nabla F(x)^T (x-x^*) - \dfrac{1}{2} \eta \| \nabla F(x) \|^2\\
		F(y) - F(x^*) \leq \dfrac{1}{2 \eta} \left( 2 \eta \nabla F(x)^T (x-x^*) - \eta^2 \| \nabla F(x) \|^2 \right)\\
		F(y) - F(x^*) \leq \dfrac{1}{2 \eta} \left( 2 \eta \nabla F(x)^T (x-x^*) - \eta^2 \| \nabla F(x) \|^2 + \| x - x^*\| - \| x - x^*\| \right)
	\end{gather*}
	Observando que $\| y - x^* \|^2 = \| x - \eta \nabla F(x) - x^* \|^2 = \| x - x^*\|^2 + \eta^2 \| \nabla F(x) \|^2 - 2 \eta \nabla F(x)^T (x - x^*)$, se llega a:
	$$ F(y) - F(x^*) \leq \dfrac{1}{2 \eta } \left( \| x - x^*\|^2 - \| y - x^*\|^2 \right) $$
	Basta aplicar la desigualdad obtenida eligiendo $x = x_{i-1}$, $y = x_{i-1} - \eta \nabla F(x_{i-1}) $ de la siguiente forma:
	\begin{align*}
		k \left( F(x_k) - F(x^*) \right) & \leq  \sum_{i=1}^{k} \left( F(x_i) - F(x^*) \right) \leq \sum_{i=1}^{k} \dfrac{1}{2 \eta } \left( \| x_{i-1} - x^*\|^2 - \| x_i - x^*\|^2 \right) = \\
		& = \dfrac{1}{2 \eta } \left( \| x_0 - x^*\|^2 - \| x_k - x^*\|^2 \right) \leq \\
		& \leq \dfrac{1}{2 \eta } \left( \| x_0 - x^*\|^2 \right)
	\end{align*}
\end{proof}

El resultado que acabamos de probar nos garantiza que, si existe un punto en el que la función objetivo alcanza su valor mínimo, el algoritmo del gradiente descendente converge.

\subsection{Aplicación en el aprendizaje}
Recordemos que las funciones de pérdida se definen como:
$$ L_{in}(w) = \dfrac{1}{N} \sum_{n=1}^{N} \ell(w, x_n, y_n)$$ 
Asumiendo que $\ell$ es diferenciable, su gradiente se expresará como:
$$ \nabla L_{in}(w) = \dfrac{1}{N} \sum_{n=1}^{N} \nabla_w \ell(w, x_n, y_n) $$

Si aplicamos el algoritmo de gradiente descendente, en cada iteración tenemos que calcular $\nabla_w \ell$ sobre cada ejemplo de entrenamiento. Lo que ocurre en la práctica, y más en el ámbito del Deep Learning es que el cálculo de los gradientes conlleva también un gran número de operaciones. Esto, sumado a que los conjuntos de datos son muy grandes, hace que la eficiencia sea un aspecto a tener en cuenta.

Por este motivo se considera una variante llamada \textbf{gradiente descendente estocástico (SGD)} que en la práctica consigue muy buenos resultados. En esta aproximación, en lugar de usar en cada iteración todo el conjunto de datos, se elige un conjunto reducido de muestras (minibatch) de forma aleatoria y se hacen las actualizaciones de esta forma. 

\begin{algorithm}
	\KwIn{Función $\ell$, tasa de aprendizaje $\eta$, conjunto $\mathcal{D}$}
	\SetAlgoLined
	Inicializar w(0)\\ 
	\For{t = 0, 1, 2, $\ldots$}{
		Escoger un minibatch $\{x_1, \ldots, x_M\}$\\
		Calcular $g(t) = \sum_{m=1}^{M} \nabla_w \ell(w(t), x_m, y_m)$\\
		$w(t+1) = w(t) - \eta g(t)$
	}
	\caption{Gradiente descendente estocástico}
\end{algorithm}

Por otra parte, debemos realizar una puntualización sobre las hipótesis bajo las cuales el gradiente descendente tiene garantizada su convergencia. En la práctica, las funciones que intentamos minimizar en aprendizaje automático y en Deep Learning no son globalmente convexas y pueden tener muchos mínimos locales que no son globales, así como puntos donde el gradiente se anula (puntos de silla) rodeados de regiones planas.

Al aplicar el gradiente descendente a estas funciones, obtenemos soluciones en los que el valor de la función de pérdida es muy bajo, pero no necesariamente óptimo.

\subsection{Variantes}
Para usar el gradiente descendente hemos de elegir el criterio de parada y la tasa de aprendizaje. La proposición \ref{prop:convergencia_grad_desc} pone de manifiesto la importancia de elegir un valor correcto para $\eta$. Por otra parte, elegir un criterio de parada erróneo puede hacer que el algoritmo oscile alrededor de un mínimo, obteniendo soluciones cada vez peores.

Existen algunas variantes del gradiente descendente (y por tanto aplicables a SGD) que adaptan estos parámetros conforme avanzan las iteraciones. Se ha comprobado empíricamente que en muchos casos su aplicación conduce a mejores resultados. A continuación, describimos brevemente la idea en que se basan algunos de ellos. 

\subsubsection{SGD con momento}
Es una técnica para acelerar la convergencia del gradiente descendente. Se basa en acumular en cada iteración $t$ un vector de velocidades $v(t)$ en las direcciones en las que más está descendiendo el valor de la función objetivo:
\begin{gather*}
	v(t+1) = \mu v(t) - \eta \nabla F(w(t))\\
	w(t+1) = w(t) + v(t+1)
\end{gather*}
siendo $\eta > 0$ la tasa de aprendizaje y $\mu \in [0,1]$ el coeficiente de momento. 

\subsubsection{Adagrad y RMSprop}
Adagrad adapta la tasa de aprendizaje a los parámetros $w$. En lugar de mantener la misma para todos, a aquellos que tienen mayores derivadas parciales les corresponde una tasa más baja. Si $w \in \mathbb{R}^d$ definimos:
\begin{gather*}
	G(0)_{i} = [ \nabla F(w(0))_{i} ]^2\\
	G(t+1)_{i} = G(t)_{i} + [ \nabla F(w(t+1))_{i} ]^2
\end{gather*}
La regla de actualización queda:
$$ w(t+1)_{i} = w(t)_{i} - \dfrac{\eta}{\sqrt{G(t)_{i} + \epsilon}} \nabla F(w(t))_{i} $$
siendo $\eta > 0$ la tasa de aprendizaje y $\epsilon > 0$ un término pequeño para evitar la división por cero.

El mayor inconveniente de Adagrad es que las componentes de $G$ en cada iteración pueden crecer indefinidamente, haciendo que las actualizaciones de los parámetros no tengan efecto. RMSProp intenta solucionar esto definiendo:
$$ G(t+1)_{i} = \alpha G(t)_{i} + (1-\alpha) [ \nabla F(w(t+1))_{i} ]^2 $$ 
para $\alpha \in [0,1]$

\subsubsection{Adam}




