{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import shfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../data/COVIDGR1.0/centralized/cropped\"\n",
    "\n",
    "args = {\"data_path\":\"../data/COVIDGR1.0-cropped\", \n",
    "        \"csv_path\": \"../partitions/partition_iid_3nodes_1.csv\",\n",
    "        \"output_path\": \"../weights\",\n",
    "        \"input_path\": \"\",\n",
    "        \"model_name\":\"transferlearning.model\", \n",
    "        \"label_bin\": \"lb.pickle\", \n",
    "        \"batch_size\": 8,\n",
    "        \"federated_rounds\": 2,\n",
    "        \"epochs_per_FL_round\": 100,\n",
    "        \"num_nodes\": 3,\n",
    "        \"size_averaging\": 1,\n",
    "        \"random_rotation\": 0,\n",
    "        \"random_shift\": 0, \n",
    "        \"random_zoom\": 0,\n",
    "        \"horizontal_flip\": False,        \n",
    "        \"finetune\": True,\n",
    "        \"train_network\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['N', 'P']\n",
    "b = ['NTN', 'NTP', 'PTP', 'PTN']\n",
    "\n",
    "lb1 = LabelBinarizer()\n",
    "lb2 = LabelBinarizer()\n",
    "\n",
    "lb1.fit(a)\n",
    "lb2.fit(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training for labels: ['N', 'P']\n",
      "[INFO] Distributing the train set across the nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/ipykernel_launcher.py:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/ipykernel_launcher.py:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] done\n"
     ]
    }
   ],
   "source": [
    "LABELS = [\"N\", \"P\"]\n",
    "print(\"[INFO] training for labels: \" + str(LABELS))\n",
    "\n",
    "print(\"[INFO] Distributing the train set across the nodes...\")\n",
    "federated_data, test_data, test_label, train_files, test_files = get_federated_data_csv(args[\"data_path\"], args[\"csv_path\"])\n",
    "print(\"[INFO] done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from shfl.private import UnprotectedAccess\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)\n",
    "print(federated_data.num_nodes())\n",
    "federated_data.configure_data_access(UnprotectedAccess())\n",
    "a = federated_data[0].query()._data\n",
    "print(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CITModel import CITModel\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def model_builder():    \n",
    "    return CITModel(LABELS, classifier_name = \"resnet18\", lambda_value = 0.00075, batch_size=args[\"batch_size\"], epochs=args[\"epochs_per_FL_round\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n",
      "[INFO] FREEZING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy round 0\n",
      "[INFO] weights = [1.         0.95714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.5119 Acc_D: 0.5219 Loss_G_class1: 3918.4218 Loss_G_class2: 4099.0678: 100%|██████████| 18/18 [00:24<00:00,  1.39s/it]\n",
      "[1/50] Loss_D: 0.3374 Acc_D: 0.5730 Loss_G_class1: 792.9886 Loss_G_class2: 938.9191: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[2/50] Loss_D: 0.3069 Acc_D: 0.6533 Loss_G_class1: 571.9469 Loss_G_class2: 713.1103: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[3/50] Loss_D: 0.2727 Acc_D: 0.7299 Loss_G_class1: 460.0826 Loss_G_class2: 593.5011: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[4/50] Loss_D: 0.2341 Acc_D: 0.7847 Loss_G_class1: 384.8871 Loss_G_class2: 507.7466: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[5/50] Loss_D: 0.1802 Acc_D: 0.8467 Loss_G_class1: 318.8187 Loss_G_class2: 428.9515: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[6/50] Loss_D: 0.1913 Acc_D: 0.8029 Loss_G_class1: 261.8874 Loss_G_class2: 362.4457: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[7/50] Loss_D: 0.1122 Acc_D: 0.9124 Loss_G_class1: 211.6623 Loss_G_class2: 304.6364: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[8/50] Loss_D: 0.1185 Acc_D: 0.8978 Loss_G_class1: 172.2732 Loss_G_class2: 255.2870: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[9/50] Loss_D: 0.1710 Acc_D: 0.8540 Loss_G_class1: 141.7219 Loss_G_class2: 213.8456: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[10/50] Loss_D: 0.1376 Acc_D: 0.8796 Loss_G_class1: 118.2869 Loss_G_class2: 178.8585: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[11/50] Loss_D: 0.0903 Acc_D: 0.9489 Loss_G_class1: 100.4672 Loss_G_class2: 149.6693: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[12/50] Loss_D: 0.0536 Acc_D: 0.9635 Loss_G_class1: 86.8882 Loss_G_class2: 126.8793: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[13/50] Loss_D: 0.0149 Acc_D: 0.9927 Loss_G_class1: 76.4175 Loss_G_class2: 109.4263: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[14/50] Loss_D: 0.0095 Acc_D: 1.0000 Loss_G_class1: 68.1994 Loss_G_class2: 95.7400: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it] \n",
      "[15/50] Loss_D: 0.0054 Acc_D: 1.0000 Loss_G_class1: 61.6166 Loss_G_class2: 84.8507: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[16/50] Loss_D: 0.0016 Acc_D: 1.0000 Loss_G_class1: 56.2269 Loss_G_class2: 76.0658: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[17/50] Loss_D: 0.0011 Acc_D: 1.0000 Loss_G_class1: 51.7177 Loss_G_class2: 68.8725: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[18/50] Loss_D: 0.0008 Acc_D: 1.0000 Loss_G_class1: 47.8704 Loss_G_class2: 62.8910: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[19/50] Loss_D: 0.0007 Acc_D: 1.0000 Loss_G_class1: 44.5324 Loss_G_class2: 57.8428: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[20/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 41.5969 Loss_G_class2: 53.5236: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[21/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 38.9862 Loss_G_class2: 49.7809: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[22/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 36.6437 Loss_G_class2: 46.4997: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[23/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 34.5250 Loss_G_class2: 43.5933: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[24/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 32.5939 Loss_G_class2: 40.9955: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[25/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 30.8197 Loss_G_class2: 38.6538: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[26/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 29.1768 Loss_G_class2: 36.5269: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[27/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 27.6461 Loss_G_class2: 34.5822: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[28/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 26.2138 Loss_G_class2: 32.7927: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[29/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 24.8693 Loss_G_class2: 31.1360: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[30/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 23.6042 Loss_G_class2: 29.5931: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[31/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 22.4117 Loss_G_class2: 28.1477: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[32/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 21.2863 Loss_G_class2: 26.7881: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[33/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 20.2228 Loss_G_class2: 25.5057: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[34/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 19.2173 Loss_G_class2: 24.2926: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[35/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 18.2681 Loss_G_class2: 23.1433: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[36/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 17.3751 Loss_G_class2: 22.0544: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[37/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 16.5353 Loss_G_class2: 21.0240: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[38/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 15.7434 Loss_G_class2: 20.0494: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[39/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 14.9948 Loss_G_class2: 19.1281: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[40/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 14.2871 Loss_G_class2: 18.2580: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[41/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 13.6181 Loss_G_class2: 17.4364: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[42/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 12.9858 Loss_G_class2: 16.6600: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[43/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 12.3885 Loss_G_class2: 15.9260: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[44/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 11.8235 Loss_G_class2: 15.2318: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[45/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 11.2875 Loss_G_class2: 14.5749: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[46/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 10.7775 Loss_G_class2: 13.9525: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.2928 Loss_G_class2: 13.3624: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.8342 Loss_G_class2: 12.8025: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.4009 Loss_G_class2: 12.2710: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.78947368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.5370 Acc_D: 0.5074 Loss_G_class1: 3862.1699 Loss_G_class2: 4038.5405: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[1/50] Loss_D: 0.3445 Acc_D: 0.5809 Loss_G_class1: 736.7531 Loss_G_class2: 910.6260: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[2/50] Loss_D: 0.3138 Acc_D: 0.6838 Loss_G_class1: 549.4995 Loss_G_class2: 690.3911: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[3/50] Loss_D: 0.2587 Acc_D: 0.7243 Loss_G_class1: 438.2554 Loss_G_class2: 568.6143: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[4/50] Loss_D: 0.2453 Acc_D: 0.7647 Loss_G_class1: 365.6222 Loss_G_class2: 483.4994: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[5/50] Loss_D: 0.2048 Acc_D: 0.8199 Loss_G_class1: 306.6174 Loss_G_class2: 410.4692: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[6/50] Loss_D: 0.1532 Acc_D: 0.9044 Loss_G_class1: 256.1705 Loss_G_class2: 348.9558: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[7/50] Loss_D: 0.1842 Acc_D: 0.8529 Loss_G_class1: 212.9769 Loss_G_class2: 295.7326: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[8/50] Loss_D: 0.0893 Acc_D: 0.9375 Loss_G_class1: 177.1072 Loss_G_class2: 250.9786: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[9/50] Loss_D: 0.0503 Acc_D: 0.9743 Loss_G_class1: 148.2530 Loss_G_class2: 213.5861: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[10/50] Loss_D: 0.0476 Acc_D: 0.9596 Loss_G_class1: 125.3280 Loss_G_class2: 182.5069: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[11/50] Loss_D: 0.0931 Acc_D: 0.9338 Loss_G_class1: 107.1683 Loss_G_class2: 156.5334: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[12/50] Loss_D: 0.1354 Acc_D: 0.8971 Loss_G_class1: 92.6458 Loss_G_class2: 134.6083: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[13/50] Loss_D: 0.1957 Acc_D: 0.8493 Loss_G_class1: 80.8818 Loss_G_class2: 116.6462: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[14/50] Loss_D: 0.1126 Acc_D: 0.9118 Loss_G_class1: 71.4394 Loss_G_class2: 102.5492: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[15/50] Loss_D: 0.0455 Acc_D: 0.9632 Loss_G_class1: 63.9699 Loss_G_class2: 91.1841: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[16/50] Loss_D: 0.0143 Acc_D: 0.9963 Loss_G_class1: 57.9602 Loss_G_class2: 81.8116: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[17/50] Loss_D: 0.0057 Acc_D: 1.0000 Loss_G_class1: 52.9839 Loss_G_class2: 73.9858: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[18/50] Loss_D: 0.0030 Acc_D: 1.0000 Loss_G_class1: 48.7715 Loss_G_class2: 67.3755: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[19/50] Loss_D: 0.0019 Acc_D: 1.0000 Loss_G_class1: 45.1465 Loss_G_class2: 61.7256: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[20/50] Loss_D: 0.0013 Acc_D: 1.0000 Loss_G_class1: 41.9860 Loss_G_class2: 56.8401: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[21/50] Loss_D: 0.0011 Acc_D: 1.0000 Loss_G_class1: 39.2016 Loss_G_class2: 52.5689: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[22/50] Loss_D: 0.0009 Acc_D: 1.0000 Loss_G_class1: 36.7270 Loss_G_class2: 48.7984: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[23/50] Loss_D: 0.0008 Acc_D: 1.0000 Loss_G_class1: 34.5097 Loss_G_class2: 45.4355: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[24/50] Loss_D: 0.0007 Acc_D: 1.0000 Loss_G_class1: 32.5065 Loss_G_class2: 42.4203: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[25/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 30.6824 Loss_G_class2: 39.7208: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[26/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 29.0084 Loss_G_class2: 37.3057: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[27/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 27.4605 Loss_G_class2: 35.1331: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[28/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 26.0197 Loss_G_class2: 33.1639: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[29/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 24.6709 Loss_G_class2: 31.3669: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[30/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 23.4026 Loss_G_class2: 29.7185: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[31/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 22.2069 Loss_G_class2: 28.1972: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[32/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 21.0784 Loss_G_class2: 26.7851: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[33/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 20.0144 Loss_G_class2: 25.4669: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[34/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 19.0121 Loss_G_class2: 24.2300: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[35/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 18.0667 Loss_G_class2: 23.0655: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[36/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 17.1734 Loss_G_class2: 21.9676: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[37/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 16.3294 Loss_G_class2: 20.9299: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[38/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 15.5333 Loss_G_class2: 19.9486: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[39/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 14.7831 Loss_G_class2: 19.0206: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[40/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 14.0759 Loss_G_class2: 18.1436: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[41/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 13.4092 Loss_G_class2: 17.3148: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[42/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 12.7798 Loss_G_class2: 16.5310: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[43/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 12.1847 Loss_G_class2: 15.7893: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[44/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 11.6215 Loss_G_class2: 15.0869: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[45/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 11.0884 Loss_G_class2: 14.4220: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[46/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 10.5839 Loss_G_class2: 13.7926: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.1065 Loss_G_class2: 13.1963: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.6541 Loss_G_class2: 12.6312: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.2249 Loss_G_class2: 12.0955: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.94285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.5428 Acc_D: 0.5441 Loss_G_class1: 4027.3152 Loss_G_class2: 4213.6659: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[1/50] Loss_D: 0.3824 Acc_D: 0.5478 Loss_G_class1: 791.9931 Loss_G_class2: 956.5468: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it] \n",
      "[2/50] Loss_D: 0.3386 Acc_D: 0.5846 Loss_G_class1: 581.1526 Loss_G_class2: 727.7830: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[3/50] Loss_D: 0.2807 Acc_D: 0.6838 Loss_G_class1: 470.3972 Loss_G_class2: 605.9912: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[4/50] Loss_D: 0.2470 Acc_D: 0.7316 Loss_G_class1: 391.3304 Loss_G_class2: 516.4154: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[5/50] Loss_D: 0.2158 Acc_D: 0.8272 Loss_G_class1: 326.6206 Loss_G_class2: 440.5982: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[6/50] Loss_D: 0.1686 Acc_D: 0.8456 Loss_G_class1: 269.0667 Loss_G_class2: 374.7675: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[7/50] Loss_D: 0.1152 Acc_D: 0.9007 Loss_G_class1: 221.2896 Loss_G_class2: 318.2720: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[8/50] Loss_D: 0.0850 Acc_D: 0.9412 Loss_G_class1: 181.8584 Loss_G_class2: 269.9785: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[9/50] Loss_D: 0.1301 Acc_D: 0.8860 Loss_G_class1: 150.3544 Loss_G_class2: 229.2716: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[10/50] Loss_D: 0.0649 Acc_D: 0.9375 Loss_G_class1: 126.0265 Loss_G_class2: 195.2519: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[11/50] Loss_D: 0.0605 Acc_D: 0.9485 Loss_G_class1: 107.2630 Loss_G_class2: 167.0634: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[12/50] Loss_D: 0.0624 Acc_D: 0.9449 Loss_G_class1: 92.7495 Loss_G_class2: 143.9968: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[13/50] Loss_D: 0.0170 Acc_D: 0.9853 Loss_G_class1: 81.4055 Loss_G_class2: 125.1975: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[14/50] Loss_D: 0.0056 Acc_D: 1.0000 Loss_G_class1: 72.3998 Loss_G_class2: 109.8048: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[15/50] Loss_D: 0.0017 Acc_D: 1.0000 Loss_G_class1: 65.1228 Loss_G_class2: 97.1097: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it] \n",
      "[16/50] Loss_D: 0.0009 Acc_D: 1.0000 Loss_G_class1: 59.1330 Loss_G_class2: 86.5434: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[17/50] Loss_D: 0.0007 Acc_D: 1.0000 Loss_G_class1: 54.1145 Loss_G_class2: 77.7152: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[18/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 49.8446 Loss_G_class2: 70.3248: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[19/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 46.1636 Loss_G_class2: 64.0878: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[20/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 42.9531 Loss_G_class2: 58.7786: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[21/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 40.1244 Loss_G_class2: 54.2180: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[22/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 37.6098 Loss_G_class2: 50.2583: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[23/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 35.3555 Loss_G_class2: 46.7824: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[24/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 33.3178 Loss_G_class2: 43.7093: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[25/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 31.4603 Loss_G_class2: 40.9827: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[26/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 29.7539 Loss_G_class2: 38.5477: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[27/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 28.1760 Loss_G_class2: 36.3521: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[28/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 26.7082 Loss_G_class2: 34.3533: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[29/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 25.3358 Loss_G_class2: 32.5192: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[30/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 24.0476 Loss_G_class2: 30.8268: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[31/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 22.8343 Loss_G_class2: 29.2583: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[32/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 21.6882 Loss_G_class2: 27.7979: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[33/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 20.6042 Loss_G_class2: 26.4317: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[34/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 19.5792 Loss_G_class2: 25.1486: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[35/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 18.6111 Loss_G_class2: 23.9394: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[36/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 17.6962 Loss_G_class2: 22.7971: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[37/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 16.8312 Loss_G_class2: 21.7165: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[38/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 16.0130 Loss_G_class2: 20.6937: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[39/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 15.2394 Loss_G_class2: 19.7251: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[40/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 14.5077 Loss_G_class2: 18.8084: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[41/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 13.8159 Loss_G_class2: 17.9420: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[42/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 13.1621 Loss_G_class2: 17.1231: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[43/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 12.5443 Loss_G_class2: 16.3486: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[44/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 11.9599 Loss_G_class2: 15.6158: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[45/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 11.4064 Loss_G_class2: 14.9224: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[46/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.8807 Loss_G_class2: 14.2658: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.3814 Loss_G_class2: 13.6438: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.9078 Loss_G_class2: 13.0544: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it] \n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.4595 Loss_G_class2: 12.4957: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [0.91549296 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.6147 Acc_D: 0.5184 Loss_G_class1: 3831.4606 Loss_G_class2: 4017.0150: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[1/50] Loss_D: 0.3984 Acc_D: 0.5441 Loss_G_class1: 759.7513 Loss_G_class2: 913.9878: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[2/50] Loss_D: 0.3353 Acc_D: 0.5882 Loss_G_class1: 561.5421 Loss_G_class2: 698.3160: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[3/50] Loss_D: 0.2702 Acc_D: 0.7316 Loss_G_class1: 448.9052 Loss_G_class2: 577.8933: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[4/50] Loss_D: 0.2261 Acc_D: 0.7831 Loss_G_class1: 375.3093 Loss_G_class2: 494.9885: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[5/50] Loss_D: 0.1992 Acc_D: 0.8235 Loss_G_class1: 313.1826 Loss_G_class2: 421.4055: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[6/50] Loss_D: 0.1654 Acc_D: 0.8456 Loss_G_class1: 258.3765 Loss_G_class2: 358.8256: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[7/50] Loss_D: 0.1558 Acc_D: 0.8603 Loss_G_class1: 212.0654 Loss_G_class2: 304.2769: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[8/50] Loss_D: 0.0855 Acc_D: 0.9191 Loss_G_class1: 175.1755 Loss_G_class2: 257.5546: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[9/50] Loss_D: 0.0685 Acc_D: 0.9449 Loss_G_class1: 145.9621 Loss_G_class2: 217.7220: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[10/50] Loss_D: 0.0475 Acc_D: 0.9669 Loss_G_class1: 123.1455 Loss_G_class2: 184.6203: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[11/50] Loss_D: 0.0359 Acc_D: 0.9743 Loss_G_class1: 105.3275 Loss_G_class2: 157.5703: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[12/50] Loss_D: 0.0449 Acc_D: 0.9743 Loss_G_class1: 91.1643 Loss_G_class2: 135.5943: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[13/50] Loss_D: 0.0859 Acc_D: 0.9338 Loss_G_class1: 79.8722 Loss_G_class2: 117.7240: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[14/50] Loss_D: 0.0432 Acc_D: 0.9632 Loss_G_class1: 71.0250 Loss_G_class2: 103.0908: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[15/50] Loss_D: 0.0314 Acc_D: 0.9706 Loss_G_class1: 63.9284 Loss_G_class2: 91.0437: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[16/50] Loss_D: 0.0226 Acc_D: 0.9779 Loss_G_class1: 58.0879 Loss_G_class2: 81.1546: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[17/50] Loss_D: 0.0165 Acc_D: 0.9926 Loss_G_class1: 53.1992 Loss_G_class2: 73.0475: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[18/50] Loss_D: 0.0032 Acc_D: 1.0000 Loss_G_class1: 49.0418 Loss_G_class2: 66.3553: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[19/50] Loss_D: 0.0013 Acc_D: 1.0000 Loss_G_class1: 45.4528 Loss_G_class2: 60.7400: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[20/50] Loss_D: 0.0008 Acc_D: 1.0000 Loss_G_class1: 42.3127 Loss_G_class2: 55.9298: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[21/50] Loss_D: 0.0007 Acc_D: 1.0000 Loss_G_class1: 39.5338 Loss_G_class2: 51.7342: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[22/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 37.0517 Loss_G_class2: 48.0219: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[23/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 34.8168 Loss_G_class2: 44.7293: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[24/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 32.7887 Loss_G_class2: 41.8167: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[25/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 30.9336 Loss_G_class2: 39.2216: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[26/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 29.2243 Loss_G_class2: 36.8836: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[27/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 27.6388 Loss_G_class2: 34.7592: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[28/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 26.1593 Loss_G_class2: 32.8197: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[29/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 24.7729 Loss_G_class2: 31.0446: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[30/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 23.4695 Loss_G_class2: 29.4118: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[31/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 22.2426 Loss_G_class2: 27.8995: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[32/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 21.0879 Loss_G_class2: 26.4899: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[33/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 20.0010 Loss_G_class2: 25.1701: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[34/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 18.9773 Loss_G_class2: 23.9305: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[35/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 18.0121 Loss_G_class2: 22.7634: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[36/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 17.1022 Loss_G_class2: 21.6631: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[37/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 16.2458 Loss_G_class2: 20.6251: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[38/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 15.4396 Loss_G_class2: 19.6446: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[39/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 14.6804 Loss_G_class2: 18.7187: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[40/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 13.9651 Loss_G_class2: 17.8458: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[41/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 13.2907 Loss_G_class2: 17.0226: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[42/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 12.6547 Loss_G_class2: 16.2454: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[43/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 12.0546 Loss_G_class2: 15.5111: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[44/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 11.4876 Loss_G_class2: 14.8172: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[45/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.9517 Loss_G_class2: 14.1608: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[46/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.4448 Loss_G_class2: 13.5397: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.9649 Loss_G_class2: 12.9518: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it] \n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.5106 Loss_G_class2: 12.3951: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.0808 Loss_G_class2: 11.8674: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [0.67901235 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.4776 Acc_D: 0.5993 Loss_G_class1: 3801.3977 Loss_G_class2: 3944.3213: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[1/50] Loss_D: 0.3457 Acc_D: 0.6066 Loss_G_class1: 757.5718 Loss_G_class2: 903.4120: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[2/50] Loss_D: 0.2925 Acc_D: 0.6949 Loss_G_class1: 545.2077 Loss_G_class2: 678.8929: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[3/50] Loss_D: 0.2483 Acc_D: 0.7574 Loss_G_class1: 443.8555 Loss_G_class2: 568.3051: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[4/50] Loss_D: 0.2549 Acc_D: 0.7610 Loss_G_class1: 367.1126 Loss_G_class2: 484.1699: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[5/50] Loss_D: 0.1856 Acc_D: 0.8382 Loss_G_class1: 304.6581 Loss_G_class2: 412.9199: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[6/50] Loss_D: 0.0982 Acc_D: 0.9265 Loss_G_class1: 250.1682 Loss_G_class2: 350.5542: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[7/50] Loss_D: 0.1099 Acc_D: 0.9081 Loss_G_class1: 205.3851 Loss_G_class2: 296.6985: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[8/50] Loss_D: 0.1235 Acc_D: 0.8934 Loss_G_class1: 169.2548 Loss_G_class2: 250.4986: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[9/50] Loss_D: 0.1189 Acc_D: 0.9007 Loss_G_class1: 140.7116 Loss_G_class2: 211.4143: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[10/50] Loss_D: 0.0825 Acc_D: 0.9522 Loss_G_class1: 118.3716 Loss_G_class2: 178.8666: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[11/50] Loss_D: 0.0319 Acc_D: 0.9816 Loss_G_class1: 100.7049 Loss_G_class2: 151.7114: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[12/50] Loss_D: 0.0088 Acc_D: 1.0000 Loss_G_class1: 87.0210 Loss_G_class2: 129.0866: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[13/50] Loss_D: 0.0025 Acc_D: 1.0000 Loss_G_class1: 76.5440 Loss_G_class2: 111.3524: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[14/50] Loss_D: 0.0010 Acc_D: 1.0000 Loss_G_class1: 68.2553 Loss_G_class2: 97.5877: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[15/50] Loss_D: 0.0007 Acc_D: 1.0000 Loss_G_class1: 61.5434 Loss_G_class2: 86.5583: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[16/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 56.0043 Loss_G_class2: 77.5610: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[17/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 51.3476 Loss_G_class2: 70.1206: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[18/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 47.3656 Loss_G_class2: 63.8851: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[19/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 43.9123 Loss_G_class2: 58.5905: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[20/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 40.8832 Loss_G_class2: 54.0418: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[21/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 38.2017 Loss_G_class2: 50.0930: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[22/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 35.8089 Loss_G_class2: 46.6289: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[23/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 33.6575 Loss_G_class2: 43.5566: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[24/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 31.7075 Loss_G_class2: 40.8060: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[25/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 29.9254 Loss_G_class2: 38.3278: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[26/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 28.2845 Loss_G_class2: 36.0869: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[27/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 26.7649 Loss_G_class2: 34.0476: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[28/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 25.3503 Loss_G_class2: 32.1788: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[29/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 24.0278 Loss_G_class2: 30.4541: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[30/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 22.7872 Loss_G_class2: 28.8533: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[31/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 21.6204 Loss_G_class2: 27.3611: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[32/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 20.5201 Loss_G_class2: 25.9670: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[33/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 19.4805 Loss_G_class2: 24.6635: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[34/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 18.4993 Loss_G_class2: 23.4431: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[35/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 17.5741 Loss_G_class2: 22.2987: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[36/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 16.7006 Loss_G_class2: 21.2234: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[37/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 15.8757 Loss_G_class2: 20.2113: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[38/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 15.0966 Loss_G_class2: 19.2571: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[39/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 14.3605 Loss_G_class2: 18.3570: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[40/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 13.6655 Loss_G_class2: 17.5079: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[41/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 13.0101 Loss_G_class2: 16.7067: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[42/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 12.3914 Loss_G_class2: 15.9498: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[43/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 11.8061 Loss_G_class2: 15.2343: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[44/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 11.2520 Loss_G_class2: 14.5577: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[45/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.7269 Loss_G_class2: 13.9173: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[46/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 10.2304 Loss_G_class2: 13.3107: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.7610 Loss_G_class2: 12.7358: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 9.3169 Loss_G_class2: 12.1906: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 8.8970 Loss_G_class2: 11.6731: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance client <shfl.private.federated_operation.FederatedDataNode object at 0x7f977d061a20>: [2.1619598865509033, 0.47368421052631576]\n",
      "Test performance client <shfl.private.federated_operation.FederatedDataNode object at 0x7f977d061a58>: [0.737774133682251, 0.4853801169590643]\n",
      "Test performance client <shfl.private.federated_operation.FederatedDataNode object at 0x7f977d061dd8>: [0.7134106159210205, 0.5087719298245614]\n",
      "Test performance client <shfl.private.federated_operation.FederatedDataNode object at 0x7f977d061e10>: [0.8111582398414612, 0.47953216374269003]\n",
      "Test performance client <shfl.private.federated_operation.FederatedDataNode object at 0x7f977d061f98>: [1.849792242050171, 0.47368421052631576]\n",
      "Global model test performance : [1.0080039501190186, 0.47368421052631576]\n",
      "\n",
      "\n",
      "\n",
      "Accuracy round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.95714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.4609 Acc_D: 0.5803 Loss_G_class1: 9.0614 Loss_G_class2: 12.1506: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[1/50] Loss_D: 0.3180 Acc_D: 0.6277 Loss_G_class1: 8.6728 Loss_G_class2: 11.6156: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[2/50] Loss_D: 0.2605 Acc_D: 0.7591 Loss_G_class1: 8.2997 Loss_G_class2: 11.0590: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[3/50] Loss_D: 0.2668 Acc_D: 0.7847 Loss_G_class1: 7.9527 Loss_G_class2: 10.6106: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[4/50] Loss_D: 0.1950 Acc_D: 0.8066 Loss_G_class1: 7.6174 Loss_G_class2: 10.1734: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[5/50] Loss_D: 0.0993 Acc_D: 0.9124 Loss_G_class1: 7.2817 Loss_G_class2: 9.7664: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[6/50] Loss_D: 0.0415 Acc_D: 0.9745 Loss_G_class1: 6.9621 Loss_G_class2: 9.3789: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[7/50] Loss_D: 0.0233 Acc_D: 0.9854 Loss_G_class1: 6.6680 Loss_G_class2: 9.0116: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[8/50] Loss_D: 0.0261 Acc_D: 0.9818 Loss_G_class1: 6.3952 Loss_G_class2: 8.6623: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[9/50] Loss_D: 0.0167 Acc_D: 0.9891 Loss_G_class1: 6.1381 Loss_G_class2: 8.3298: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[10/50] Loss_D: 0.0083 Acc_D: 1.0000 Loss_G_class1: 5.8940 Loss_G_class2: 8.0130: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[11/50] Loss_D: 0.0027 Acc_D: 1.0000 Loss_G_class1: 5.6613 Loss_G_class2: 7.7108: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[12/50] Loss_D: 0.0011 Acc_D: 1.0000 Loss_G_class1: 5.4386 Loss_G_class2: 7.4224: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[13/50] Loss_D: 0.0007 Acc_D: 1.0000 Loss_G_class1: 5.2245 Loss_G_class2: 7.1471: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[14/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 5.0184 Loss_G_class2: 6.8841: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[15/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 4.8208 Loss_G_class2: 6.6327: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[16/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 4.6324 Loss_G_class2: 6.3924: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[17/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 4.4532 Loss_G_class2: 6.1625: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[18/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 4.2831 Loss_G_class2: 5.9424: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[19/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 4.1216 Loss_G_class2: 5.7320: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[20/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 3.9676 Loss_G_class2: 5.5306: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[21/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.8203 Loss_G_class2: 5.3379: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[22/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.6790 Loss_G_class2: 5.1535: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[23/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.5436 Loss_G_class2: 4.9769: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[24/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.4140 Loss_G_class2: 4.8074: 100%|██████████| 18/18 [00:24<00:00,  1.38s/it]\n",
      "[25/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.2901 Loss_G_class2: 4.6445: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[26/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.1719 Loss_G_class2: 4.4878: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[27/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.0591 Loss_G_class2: 4.3369: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[28/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.9517 Loss_G_class2: 4.1917: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[29/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.8494 Loss_G_class2: 4.0522: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[30/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.7518 Loss_G_class2: 3.9180: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[31/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.6587 Loss_G_class2: 3.7888: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[32/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.5700 Loss_G_class2: 3.6639: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[33/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.4858 Loss_G_class2: 3.5431: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[34/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.4059 Loss_G_class2: 3.4261: 100%|██████████| 18/18 [00:24<00:00,  1.36s/it]\n",
      "[35/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.3303 Loss_G_class2: 3.3130: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[36/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.2583 Loss_G_class2: 3.2039: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[37/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.1883 Loss_G_class2: 3.0990: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[38/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.1181 Loss_G_class2: 2.9982: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[39/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.0494 Loss_G_class2: 2.9014: 100%|██████████| 18/18 [00:24<00:00,  1.38s/it]\n",
      "[40/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 1.9880 Loss_G_class2: 2.8084: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[41/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 1.9369 Loss_G_class2: 2.7191: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[42/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 1.8923 Loss_G_class2: 2.6333: 100%|██████████| 18/18 [00:24<00:00,  1.38s/it]\n",
      "[43/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 1.8383 Loss_G_class2: 2.5510: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[44/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 1.7707 Loss_G_class2: 2.4719: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[45/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.7101 Loss_G_class2: 2.3961: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[46/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.6578 Loss_G_class2: 2.3235: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.6141 Loss_G_class2: 2.2540: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.5757 Loss_G_class2: 2.1874: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.5372 Loss_G_class2: 2.1237: 100%|██████████| 18/18 [00:24<00:00,  1.37s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.78947368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.4513 Acc_D: 0.5588 Loss_G_class1: 8.6641 Loss_G_class2: 11.6545: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[1/50] Loss_D: 0.4069 Acc_D: 0.5772 Loss_G_class1: 8.2749 Loss_G_class2: 11.1131: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[2/50] Loss_D: 0.2353 Acc_D: 0.7684 Loss_G_class1: 7.9124 Loss_G_class2: 10.6069: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[3/50] Loss_D: 0.2442 Acc_D: 0.7904 Loss_G_class1: 7.5643 Loss_G_class2: 10.1682: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[4/50] Loss_D: 0.1821 Acc_D: 0.8162 Loss_G_class1: 7.2401 Loss_G_class2: 9.7533: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[5/50] Loss_D: 0.0778 Acc_D: 0.9779 Loss_G_class1: 6.9327 Loss_G_class2: 9.3589: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[6/50] Loss_D: 0.0163 Acc_D: 0.9926 Loss_G_class1: 6.6419 Loss_G_class2: 8.9854: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[7/50] Loss_D: 0.0083 Acc_D: 1.0000 Loss_G_class1: 6.3667 Loss_G_class2: 8.6297: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[8/50] Loss_D: 0.0047 Acc_D: 0.9963 Loss_G_class1: 6.1056 Loss_G_class2: 8.2910: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[9/50] Loss_D: 0.0112 Acc_D: 0.9890 Loss_G_class1: 5.8575 Loss_G_class2: 7.9682: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[10/50] Loss_D: 0.0236 Acc_D: 0.9853 Loss_G_class1: 5.6212 Loss_G_class2: 7.6605: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[11/50] Loss_D: 0.0243 Acc_D: 0.9779 Loss_G_class1: 5.3955 Loss_G_class2: 7.3670: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[12/50] Loss_D: 0.0127 Acc_D: 0.9853 Loss_G_class1: 5.1801 Loss_G_class2: 7.0870: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[13/50] Loss_D: 0.0039 Acc_D: 1.0000 Loss_G_class1: 4.9757 Loss_G_class2: 6.8197: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[14/50] Loss_D: 0.0013 Acc_D: 1.0000 Loss_G_class1: 4.7820 Loss_G_class2: 6.5645: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[15/50] Loss_D: 0.0006 Acc_D: 1.0000 Loss_G_class1: 4.5982 Loss_G_class2: 6.3207: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[16/50] Loss_D: 0.0005 Acc_D: 1.0000 Loss_G_class1: 4.4232 Loss_G_class2: 6.0875: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[17/50] Loss_D: 0.0004 Acc_D: 1.0000 Loss_G_class1: 4.2567 Loss_G_class2: 5.8644: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[18/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 4.0984 Loss_G_class2: 5.6509: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[19/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.9478 Loss_G_class2: 5.4463: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[20/50] Loss_D: 0.0003 Acc_D: 1.0000 Loss_G_class1: 3.8036 Loss_G_class2: 5.2503: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[21/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 3.6649 Loss_G_class2: 5.0625: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[22/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 3.5312 Loss_G_class2: 4.8825: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[23/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 3.4026 Loss_G_class2: 4.7100: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[24/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 3.2796 Loss_G_class2: 4.5447: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[25/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 3.1625 Loss_G_class2: 4.3862: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[26/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 3.0510 Loss_G_class2: 4.2343: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[27/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.9444 Loss_G_class2: 4.0885: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[28/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.8417 Loss_G_class2: 3.9485: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[29/50] Loss_D: 0.0002 Acc_D: 1.0000 Loss_G_class1: 2.7431 Loss_G_class2: 3.8139: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[30/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.6493 Loss_G_class2: 3.6845: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[31/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.5605 Loss_G_class2: 3.5601: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[32/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.4763 Loss_G_class2: 3.4408: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[33/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.3962 Loss_G_class2: 3.3266: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[34/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.3199 Loss_G_class2: 3.2177: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[35/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.2471 Loss_G_class2: 3.1144: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[36/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.1760 Loss_G_class2: 3.0166: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[37/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.1047 Loss_G_class2: 2.9216: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[38/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 2.0353 Loss_G_class2: 2.8264: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[39/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.9698 Loss_G_class2: 2.7325: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[40/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.9090 Loss_G_class2: 2.6436: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[41/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.8524 Loss_G_class2: 2.5598: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[42/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.7989 Loss_G_class2: 2.4802: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[43/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.7470 Loss_G_class2: 2.4044: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[44/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.6966 Loss_G_class2: 2.3324: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[45/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.6483 Loss_G_class2: 2.2632: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[46/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.6028 Loss_G_class2: 2.1949: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[47/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.5601 Loss_G_class2: 2.1280: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[48/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.5193 Loss_G_class2: 2.0640: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[49/50] Loss_D: 0.0001 Acc_D: 1.0000 Loss_G_class1: 1.4797 Loss_G_class2: 2.0032: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.94285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0/50] Loss_D: 0.4150 Acc_D: 0.5882 Loss_G_class1: 9.2934 Loss_G_class2: 12.3981: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[1/50] Loss_D: 0.3384 Acc_D: 0.7132 Loss_G_class1: 8.8768 Loss_G_class2: 11.7947: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[2/50] Loss_D: 0.2709 Acc_D: 0.7794 Loss_G_class1: 8.4736 Loss_G_class2: 11.2852: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[3/50] Loss_D: 0.1978 Acc_D: 0.8640 Loss_G_class1: 8.0964 Loss_G_class2: 10.8032: 100%|██████████| 17/17 [00:24<00:00,  1.44s/it]\n",
      "[4/50] Loss_D: 0.1007 Acc_D: 0.9338 Loss_G_class1: 7.7408 Loss_G_class2: 10.3569: 100%|██████████| 17/17 [00:24<00:00,  1.43s/it]\n",
      "[5/50] Loss_D: 0.0733 Acc_D: 0.9286 Loss_G_class1: 7.5786 Loss_G_class2: 10.1693:  82%|████████▏ | 14/17 [00:20<00:04,  1.41s/it]"
     ]
    }
   ],
   "source": [
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "federated_government.run_rounds(args[\"federated_rounds\"], test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.private import UnprotectedAccess\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def sample_loader(sample):\n",
    "    loader = transforms.Compose([transforms.ToTensor()])\n",
    "    s = loader(sample).float()\n",
    "    s = Variable(s, requires_grad=False)\n",
    "    s = s.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return s.to(device) #assumes that you're using GPU\n",
    "\n",
    "def show_img(G_dict, sample):\n",
    "    x = sample_loader(sample)\n",
    "    class_name = LABELS[0]\n",
    "    y = G_dict[class_name](x)\n",
    "    y = ToPILImage()(y[0].cpu().detach())\n",
    "    #y.save(\"./prueba.png\")\n",
    "    display(y)\n",
    "    \n",
    "def transform_data(G_dict, class_names, data, labels):\n",
    "    new_labels = []\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        sample = data[i]\n",
    "        label = lb1.inverse_transform(labels[i])[0]\n",
    "        x = sample_loader(sample)\n",
    "        for class_name in class_names:\n",
    "            y = G_dict[class_name](x)\n",
    "            y = y[0].cpu().detach().numpy()\n",
    "            y = np.moveaxis(y, 0, -1)\n",
    "            new_data.append(y)\n",
    "            new_label = str(label) + \"T\" + class_name\n",
    "            new_labels.append(new_label)\n",
    "    new_labels = lb2.transform(new_labels)\n",
    "    \n",
    "    return np.asarray(new_data), np.asarray(new_labels)\n",
    "\n",
    "\n",
    "G_dict = federated_government.global_model._G_dict\n",
    "for class_name in LABELS:\n",
    "    G_dict[class_name]= G_dict[class_name].to(device)\n",
    "federated_data.configure_data_access(UnprotectedAccess())\n",
    "\n",
    "new_federated_data = copy.deepcopy(federated_data)\n",
    "\n",
    "for i in range(federated_data.num_nodes()):\n",
    "    data_node = federated_data[i]\n",
    "    new_data_node = new_federated_data[i]\n",
    "    data = data_node.query()._data\n",
    "    labels = data_node.query()._label\n",
    "    new_data, new_labels = transform_data(G_dict, LABELS, data, labels)\n",
    "    new_data_node.query()._data = new_data\n",
    "    new_data_node.query()._label = new_labels\n",
    "    \n",
    "    #print(data_node.query()._label)\n",
    "    #print(new_data_node.query()._label)\n",
    "    #data_node.query()._data = new_data\n",
    "    #data_node.query()._label = new_labels\n",
    "\n",
    "new_test_data, new_test_label = transform_data(G_dict, LABELS, test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "datagen_train = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input,\n",
    "    rotation_range = args[\"random_rotation\"],\n",
    "    width_shift_range = args[\"random_shift\"],\n",
    "    height_shift_range = args[\"random_shift\"],\n",
    "    zoom_range = args[\"random_zoom\"],\n",
    "    horizontal_flip = args[\"horizontal_flip\"]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input, validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "class TransferLearningModel(shfl.model.DeepLearningModel):    \n",
    "    \n",
    "    def train(self, data, labels):\n",
    "        train_generator = train_datagen.flow(data, labels, batch_size=args[\"batch_size\"], subset='training')\n",
    "\n",
    "        validation_generator = train_datagen.flow(data, labels, batch_size=args[\"batch_size\"], subset='validation')\n",
    "        #self._check_data(data)\n",
    "        #self._check_labels(labels)\n",
    "\n",
    "        #early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "        self._model.fit(\n",
    "            x=train_generator,\n",
    "            steps_per_epoch= int(len(data)*0.8) // args[\"batch_size\"],\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps = int(len(data)*0.2) // args[\"batch_size\"],\n",
    "            epochs=self._epochs\n",
    "        )\n",
    "\n",
    "def model_builder_2():\n",
    "    \n",
    "    resnet50 = tf.keras.applications.ResNet50(include_top = False, weights = 'imagenet', pooling = 'avg', input_tensor=Input(shape=(256, 256, 3)))\n",
    "    \n",
    "    if (args[\"finetune\"]):\n",
    "        resnet50.trainable = False\n",
    "    else: \n",
    "        resnet50.trainable = True\n",
    "    \n",
    "    # Add last layers\n",
    "    x = resnet50.output\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    predictions = tf.keras.layers.Dense(4, activation = 'softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = resnet50.input, outputs = predictions)\n",
    "    \n",
    "    criterion = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.SGD(lr = 1e-3, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "    metrics = [tf.keras.metrics.categorical_accuracy]\n",
    "    \n",
    "    return TransferLearningModel(model=model, criterion=criterion, optimizer=optimizer, metrics=metrics, epochs=args[\"epochs_per_FL_round\"], batch_size = args[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Train the network:\n",
    "epochs_per_FL_round=args[\"epochs_per_FL_round\"]\n",
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "new_federated_government = shfl.federated_government.FederatedGovernment(model_builder_2, new_federated_data, aggregator)\n",
    "new_federated_government.run_rounds(args[\"federated_rounds\"], new_test_data, new_test_label)\n",
    "#print(\"[INFO] saving model ...\")\n",
    "#federated_government.global_model._model.save( os.path.join(args[\"output_path\"], args[\"model_name\"]) )\n",
    "print(\"[INFO] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = []\n",
    "preds = []\n",
    "no_concuerda = 0\n",
    "preds_4 = []\n",
    "true_labels_4 = []\n",
    "tabla_preds = np.empty((len(test_files), 3), dtype = '<U50')\n",
    "\n",
    "model = new_federated_government.global_model._model\n",
    "\n",
    "dict_labels = { 'PTP' : np.argmax(lb2.transform(['PTP'])[0]) , 'PTN' : np.argmax(lb2.transform(['PTN'])[0]) , \n",
    "                'NTP' : np.argmax(lb2.transform(['NTP'])[0]) , 'NTN' : np.argmax(lb2.transform(['NTN'])[0])\n",
    "              } \n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    image_path = test_files[0]\n",
    "    name = image_path.split(os.path.sep)[-1].split('.')[0]\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    \n",
    "    true_labels.append(label)\n",
    "    true_labels_4.append(dict_labels[label + \"TP\"])\n",
    "    true_labels_4.append(dict_labels[label + \"TN\"])\n",
    "    tabla_preds[i,0] = name\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (256, 256))\n",
    "    \n",
    "    x = sample_loader(image)\n",
    "    tp = G_dict['P'](x)\n",
    "    tp = tp[0].cpu().detach().numpy()\n",
    "    tp = np.moveaxis(tp, 0, -1)\n",
    "    \n",
    "    tn = G_dict['N'](x)\n",
    "    tn = tn[0].cpu().detach().numpy()\n",
    "    tn = np.moveaxis(tn, 0, -1)\n",
    "\n",
    "    tp = np.expand_dims(tp, axis = 0)\n",
    "    tn = np.expand_dims(tn, axis = 0)\n",
    "    tp = preprocess_input(tp)\n",
    "    tn = preprocess_input(tn)\n",
    "\n",
    "    prob_tp = model.predict(tp)\n",
    "    prob_tn = model.predict(tn)\n",
    "    \n",
    "    pred_tp = np.argmax(prob_tp)\n",
    "    pred_tn = np.argmax(prob_tn)\n",
    "    preds_4.append(pred_tp)\n",
    "    preds_4.append(pred_tn)\n",
    "    \n",
    "    \n",
    "    # print('prediccion tp: ' + str(pred_tp))\n",
    "    # print('prediccion tn: ' + str(pred_tn))\n",
    "\n",
    "    if pred_tp == dict_labels['NTP'] and pred_tn == dict_labels['NTN']:\n",
    "        pred = 'N'\n",
    "    elif pred_tp == dict_labels['PTP'] and pred_tn == dict_labels['PTN']:\n",
    "        pred = 'P'\n",
    "    else:\n",
    "        no_concuerda = no_concuerda + 1\n",
    "        # prob_p = prob_tp[0][dict['PTP']] + prob_tp[0][dict['PTN']] + prob_tn[0][dict['PTP']] + prob_tn[0][dict['PTN']]\n",
    "        # prob_n = prob_tp[0][dict['NTP']] + prob_tp[0][dict['NTN']] + prob_tn[0][dict['NTP']] + prob_tn[0][dict['NTN']]\n",
    "        prob_p = max(prob_tp[0][dict_labels['PTP']], prob_tp[0][dict_labels['PTN']], prob_tn[0][dict_labels['PTP']], prob_tn[0][dict_labels['PTN']])\n",
    "        prob_n = max(prob_tp[0][dict_labels['NTP']], prob_tp[0][dict_labels['NTN']], prob_tn[0][dict_labels['NTP']], prob_tn[0][dict_labels['NTN']])\n",
    "        if prob_p >= prob_n:\n",
    "            pred = 'P'\n",
    "        else:\n",
    "            pred = 'N'\n",
    "\n",
    "    preds.append(pred)\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "preds = np.array(preds)\n",
    "true_labels_4 = np.array(true_labels_4)\n",
    "preds_4 = np.array(preds_4)\n",
    "\n",
    "print(preds)\n",
    "print(true_labels)\n",
    "\n",
    "tabla_preds[:,1] = true_labels\n",
    "tabla_preds[:,2] = preds\n",
    "#np.savetxt(save_preds_file, tabla_preds, fmt = '%1s', delimiter = ',')\n",
    "\n",
    "# Calculate accuracy\n",
    "acc_4 = sum(true_labels_4 == preds_4)/len(true_labels_4)\n",
    "print('Accuracy 4 clases: ' + str(acc_4))\n",
    "print('Numero de veces no concuerda: ' + str(no_concuerda))\n",
    "acc = sum(true_labels == preds)/len(true_labels)\n",
    "results = classification_report(true_labels, preds, digits = 5, output_dict = True)\n",
    "\n",
    "if results['N']['recall'] >= 0.73 and results['P']['recall'] >= 0.73:\n",
    "    model.save(save_model_file)\n",
    "\n",
    "print(results)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}