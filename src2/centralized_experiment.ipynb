{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import shfl\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from shfl.private import UnprotectedAccess\n",
    "from CIT.model import CITModel\n",
    "from utils import get_federated_data_csv, get_data_csv\n",
    "from ClassifierModel import ClassifierModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../data/COVIDGR1.0/centralized/cropped\"\n",
    "#partition_iid_3nodes_1.csv\n",
    "args = {\"data_path\":\"../data/COVIDGR1.0-Segmentadas\", \n",
    "        \"csv_path\": \"../partitions/partition_iid_1nodes_1.csv\",\n",
    "        \"output_path\": \"../weights\",\n",
    "        \"input_path\": \"\",\n",
    "        \"model_name\":\"transferlearning.model\", \n",
    "        \"label_bin\": \"lb.pickle\", \n",
    "        \"batch_size\": 8,\n",
    "        \"federated_rounds\": 1,\n",
    "        \"epochs_per_FL_round\": 100,\n",
    "        \"num_nodes\": 3,\n",
    "        \"size_averaging\": 1,\n",
    "        \"random_rotation\": 0,\n",
    "        \"random_shift\": 0, \n",
    "        \"random_zoom\": 0,\n",
    "        \"horizontal_flip\": False,        \n",
    "        \"finetune\": True,\n",
    "        \"train_network\": True}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cit_builder():    \n",
    "    return CITModel(['N', 'P'], classifier_name = \"resnet18\", lambda_values = [0.05], batch_size=args[\"batch_size\"], epochs=args[\"epochs_per_FL_round\"], device=device)\n",
    "\n",
    "def classifier_builder():\n",
    "    return ClassifierModel(batch_size=args[\"batch_size\"], epochs=args[\"epochs_per_FL_round\"], finetune = args[\"finetune\"])\n",
    "\n",
    "def get_transformed_data(federated_data, cit_federated_government, lb1, lb2):\n",
    "    t_federated_data = copy.deepcopy(federated_data)\n",
    "\n",
    "    for i in range(federated_data.num_nodes()):\n",
    "        data_node = federated_data[i]\n",
    "        t_data_node = t_federated_data[i]\n",
    "        data = data_node.query()._data\n",
    "        labels = data_node.query()._label\n",
    "        t_data, t_labels = cit_federated_government.global_model.transform_data(data, labels, lb1, lb2)\n",
    "        t_data_node.query()._data = t_data\n",
    "        t_data_node.query()._label = t_labels\n",
    "\n",
    "    t_test_data, t_test_label = cit_federated_government.global_model.transform_data(test_data, test_label, lb1, lb2)\n",
    "\n",
    "    return t_federated_data, t_test_data, t_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "a = ['N', 'P']\n",
    "b = ['NTN', 'NTP', 'PTP', 'PTN']\n",
    "lb1 = LabelBinarizer()\n",
    "lb2 = LabelBinarizer()\n",
    "lb1.fit(a)\n",
    "lb2.fit(b)\n",
    "\n",
    "data, label, train_data, train_label, test_data, test_label, train_files, test_files = get_data_csv(args[\"data_path\"], args[\"csv_path\"], lb1)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.97391304]\n",
      "LAMBDA: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/100] Loss_D: 0.3582 Acc_D: 0.6348 Loss_G_class1: 0.0610 Loss_G_class2: 0.0593: 100%|██████████| 77/77 [01:47<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.6812: 100%|██████████| 69/69 [00:01<00:00, 49.52it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.6811594202898551\n",
      "Valid Loss = 0.8394831019467202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/100] Loss_D: 0.2946 Acc_D: 0.7059 Loss_G_class1: 0.0182 Loss_G_class2: 0.0185: 100%|██████████| 77/77 [01:47<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.7101: 100%|██████████| 69/69 [00:01<00:00, 45.43it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7101449275362319\n",
      "Valid Loss = 0.512138784525619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/100] Loss_D: 0.2860 Acc_D: 0.7124 Loss_G_class1: 0.0175 Loss_G_class2: 0.0180: 100%|██████████| 77/77 [01:48<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.7681: 100%|██████████| 69/69 [00:01<00:00, 44.45it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7681159420289855\n",
      "Valid Loss = 0.5689974510907263\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/100] Loss_D: 0.2693 Acc_D: 0.7484 Loss_G_class1: 0.0164 Loss_G_class2: 0.0161: 100%|██████████| 77/77 [01:47<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.6957: 100%|██████████| 69/69 [00:01<00:00, 43.39it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.6956521739130435\n",
      "Valid Loss = 0.525682557752167\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/100] Loss_D: 0.2659 Acc_D: 0.7525 Loss_G_class1: 0.0157 Loss_G_class2: 0.0164: 100%|██████████| 77/77 [01:48<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.7391: 100%|██████████| 69/69 [00:01<00:00, 46.44it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7391304347826086\n",
      "Valid Loss = 0.5728574082579302\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/100] Loss_D: 0.2318 Acc_D: 0.7835 Loss_G_class1: 0.0119 Loss_G_class2: 0.0144: 100%|██████████| 77/77 [01:48<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.8406: 100%|██████████| 69/69 [00:01<00:00, 48.80it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.8405797101449275\n",
      "Valid Loss = 0.4669472010995167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/100] Loss_D: 0.2188 Acc_D: 0.8007 Loss_G_class1: 0.0124 Loss_G_class2: 0.0129: 100%|██████████| 77/77 [01:48<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.7681: 100%|██████████| 69/69 [00:01<00:00, 45.13it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7681159420289855\n",
      "Valid Loss = 0.48391019690619863\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/100] Loss_D: 0.2226 Acc_D: 0.7966 Loss_G_class1: 0.0122 Loss_G_class2: 0.0135: 100%|██████████| 77/77 [01:48<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.7536: 100%|██████████| 69/69 [00:01<00:00, 47.98it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7536231884057971\n",
      "Valid Loss = 0.47884143679065333\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/100] Loss_D: 0.2108 Acc_D: 0.8031 Loss_G_class1: 0.0119 Loss_G_class2: 0.0120: 100%|██████████| 77/77 [01:47<00:00,  1.40s/it]\n",
      "[Validating]: Acc_D: 0.6957: 100%|██████████| 69/69 [00:01<00:00, 43.45it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.6956521739130435\n",
      "Valid Loss = 0.5718406884917531\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/100] Loss_D: 0.1749 Acc_D: 0.8542 Loss_G_class1: 0.0088 Loss_G_class2: 0.0118:  43%|████▎     | 33/77 [00:46<01:02,  1.41s/it]"
     ]
    }
   ],
   "source": [
    "cit_model = cit_builder()\n",
    "cit_model.train(train_data, train_label)\n",
    "\n",
    "#t_train_data, t_train_label = cit_model.transform_data(train_data, train_label, lb1, lb2)\n",
    "#t_test_data, t_test_label = cit_model.transform_data(test_data, test_label, lb1, lb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "\n",
    "\n",
    "def sample_loader(sample):\n",
    "    s = ToTensor()(x).float()\n",
    "    s = Variable(s, requires_grad=False)\n",
    "    s = s.unsqueeze(0)  \n",
    "    return s\n",
    "\n",
    "self = cit_model\n",
    "\n",
    "for class_name in self._class_names:\n",
    "    self._G_dict[class_name]= self._G_dict[class_name].to(self._device)\n",
    "\n",
    "\n",
    "sample = test_data[40]\n",
    "label = lb1.inverse_transform(test_label[0])[0]\n",
    "x = ToTensor()(sample).float().unsqueeze(0).to(device)\n",
    "class_name = 'P'\n",
    "y = self._G_dict[class_name](x)\n",
    "y = y[0].cpu().detach().numpy()\n",
    "print(y.shape)\n",
    "y = np.moveaxis(y, 0, -1)\n",
    "#y = cv2.resize(y, dsize=(224, 224))\n",
    "plt.imshow(y)\n",
    "plt.imshow(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = cit_model.evaluate(test_data, test_label)\n",
    "print(\"CIT Classifier Results:\")\n",
    "print(\"Loss: {}\".format(metrics[0]))\n",
    "print(\"Acc: {}\".format(metrics[1]))\n",
    "print(\"F1: {}\".format(metrics[2]))\n",
    "print(\"Precision: {}\".format(metrics[3]))\n",
    "print(\"Recall: {}\".format(metrics[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data, t_train_label = cit_model.transform_data(train_data, train_label, lb1, lb2)\n",
    "t_test_data, t_test_label = cit_model.transform_data(test_data, test_label, lb1, lb2)\n",
    "\n",
    "from ClassifierModel import ClassifierModel\n",
    "\n",
    "classifier_model = classifier_builder()\n",
    "classifier_model.train(t_train_data, t_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = { 'PTP' : np.argmax(lb2.transform(['PTP'])[0]) , 'PTN' : np.argmax(lb2.transform(['PTN'])[0]) , \n",
    "                'NTP' : np.argmax(lb2.transform(['NTP'])[0]) , 'NTN' : np.argmax(lb2.transform(['NTN'])[0])\n",
    "              } \n",
    "G_dict = cit_model._G_dict\n",
    "\n",
    "for key, _ in G_dict.items():\n",
    "    G_dict[key].to(\"cpu\")\n",
    "\n",
    "classifier_model.get_classification_report(test_files, dict_labels, G_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
