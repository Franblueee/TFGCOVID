{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "import shfl\n",
    "import torch\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from shfl.private import UnprotectedAccess\n",
    "from CIT.model import CITModel\n",
    "from utils import get_federated_data_csv, get_data_csv\n",
    "from ClassifierModel import ClassifierModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../data/COVIDGR1.0/centralized/cropped\"\n",
    "#partition_iid_1nodes_1.csv\n",
    "args = {\"data_path\":\"../data/COVIDGR1.0-Segmentadas\", \n",
    "        \"csv_path\": \"../partitions/partition_iid_1nodes_1.csv\",\n",
    "        \"output_path\": \"../weights\",\n",
    "        \"input_path\": \"\",\n",
    "        \"model_name\":\"transferlearning.model\", \n",
    "        \"label_bin\": \"lb.pickle\", \n",
    "        \"batch_size\": 8,\n",
    "        \"federated_rounds\": 1,\n",
    "        \"epochs_per_FL_round\": 20,\n",
    "        \"num_nodes\": 3,\n",
    "        \"size_averaging\": 1,\n",
    "        \"random_rotation\": 0,\n",
    "        \"random_shift\": 0, \n",
    "        \"random_zoom\": 0,\n",
    "        \"horizontal_flip\": False,        \n",
    "        \"finetune\": True,\n",
    "        \"train_network\": True}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "a = ['N', 'P']\n",
    "b = ['NTN', 'NTP', 'PTP', 'PTN']\n",
    "lb1 = LabelBinarizer()\n",
    "lb2 = LabelBinarizer()\n",
    "lb1.fit(a)\n",
    "lb2.fit(b)\n",
    "\n",
    "dict_labels = { 'PTP' : np.argmax(lb2.transform(['PTP'])[0]) , 'PTN' : np.argmax(lb2.transform(['PTN'])[0]) , \n",
    "                'NTP' : np.argmax(lb2.transform(['NTP'])[0]) , 'NTN' : np.argmax(lb2.transform(['NTN'])[0]), \n",
    "                'P' : lb1.transform(['P'])[0][0], 'N' : lb1.transform(['N'])[0][0]\n",
    "              } \n",
    "\n",
    "data, label, train_data, train_label, test_data, test_label, train_files, test_files = get_data_csv(args[\"data_path\"], args[\"csv_path\"], lb1)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cit_builder():    \n",
    "    return CITModel(['N', 'P'], classifier_name = \"resnet18\", folds=1, lambda_values = [0.05], batch_size=args[\"batch_size\"], epochs=args[\"epochs_per_FL_round\"], device=device)\n",
    "\n",
    "def classifier_builder( G_dict ):\n",
    "    return ClassifierModel(G_dict, dict_labels, batch_size=args[\"batch_size\"], epochs=args[\"epochs_per_FL_round\"], finetune = args[\"finetune\"])\n",
    "\n",
    "def get_transformed_data(federated_data, cit_federated_government, lb1, lb2):\n",
    "    t_federated_data = copy.deepcopy(federated_data)\n",
    "\n",
    "    for i in range(federated_data.num_nodes()):\n",
    "        data_node = federated_data[i]\n",
    "        t_data_node = t_federated_data[i]\n",
    "        data = data_node.query()._data\n",
    "        labels = data_node.query()._label\n",
    "        t_data, t_labels = cit_federated_government.global_model.transform_data(data, labels, lb1, lb2)\n",
    "        t_data_node.query()._data = t_data\n",
    "        t_data_node.query()._label = t_labels\n",
    "\n",
    "    t_test_data, t_test_label = cit_federated_government.global_model.transform_data(test_data, test_label, lb1, lb2)\n",
    "\n",
    "    return t_federated_data, t_test_data, t_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] weights = [1.         0.97391304]\n",
      "[INFO] LAMBDA: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating]: Acc_D: 0.4783: 100%|██████████| 69/69 [00:02<00:00, 33.90it/s]\n",
      "[1/20] Loss_D: 0.3630 Acc_D: 0.6291 Loss_G_class1: 0.0606 Loss_G_class2: 0.0498: 100%|██████████| 77/77 [01:50<00:00,  1.44s/it]\n",
      "[Validating]: Acc_D: 0.6957: 100%|██████████| 69/69 [00:01<00:00, 40.84it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.6956521739130435\n",
      "Valid Loss = 0.5250581140103547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/20] Loss_D: 0.3020 Acc_D: 0.6993 Loss_G_class1: 0.0184 Loss_G_class2: 0.0184: 100%|██████████| 77/77 [01:50<00:00,  1.44s/it]\n",
      "[Validating]: Acc_D: 0.6957: 100%|██████████| 69/69 [00:01<00:00, 38.45it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.6956521739130435\n",
      "Valid Loss = 0.6158042478280655\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/20] Loss_D: 0.2839 Acc_D: 0.7075 Loss_G_class1: 0.0163 Loss_G_class2: 0.0171: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.7391: 100%|██████████| 69/69 [00:01<00:00, 37.92it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7391304347826086\n",
      "Valid Loss = 0.6361405757134375\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/20] Loss_D: 0.2769 Acc_D: 0.7206 Loss_G_class1: 0.0157 Loss_G_class2: 0.0166: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.7246: 100%|██████████| 69/69 [00:01<00:00, 38.25it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7246376811594203\n",
      "Valid Loss = 0.5989833228101117\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/20] Loss_D: 0.2759 Acc_D: 0.7263 Loss_G_class1: 0.0161 Loss_G_class2: 0.0160: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.6087: 100%|██████████| 69/69 [00:01<00:00, 36.17it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.6086956521739131\n",
      "Valid Loss = 0.616572426578057\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/20] Loss_D: 0.2465 Acc_D: 0.7851 Loss_G_class1: 0.0156 Loss_G_class2: 0.0128: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.7246: 100%|██████████| 69/69 [00:01<00:00, 37.91it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7246376811594203\n",
      "Valid Loss = 0.6402413869897524\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/20] Loss_D: 0.2228 Acc_D: 0.8031 Loss_G_class1: 0.0122 Loss_G_class2: 0.0124: 100%|██████████| 77/77 [01:50<00:00,  1.44s/it]\n",
      "[Validating]: Acc_D: 0.7101: 100%|██████████| 69/69 [00:01<00:00, 38.71it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7101449275362319\n",
      "Valid Loss = 0.6446031386545603\n",
      "EarlyStopping counter: 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/20] Loss_D: 0.2068 Acc_D: 0.8064 Loss_G_class1: 0.0113 Loss_G_class2: 0.0116: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.7826: 100%|██████████| 69/69 [00:01<00:00, 38.06it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.782608695652174\n",
      "Valid Loss = 0.5841134913481664\n",
      "EarlyStopping counter: 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/20] Loss_D: 0.2180 Acc_D: 0.7900 Loss_G_class1: 0.0116 Loss_G_class2: 0.0124: 100%|██████████| 77/77 [01:50<00:00,  1.44s/it]\n",
      "[Validating]: Acc_D: 0.7101: 100%|██████████| 69/69 [00:01<00:00, 36.72it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7101449275362319\n",
      "Valid Loss = 0.6241113209854001\n",
      "EarlyStopping counter: 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/20] Loss_D: 0.1983 Acc_D: 0.8235 Loss_G_class1: 0.0108 Loss_G_class2: 0.0113: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.7246: 100%|██████████| 69/69 [00:01<00:00, 37.40it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7246376811594203\n",
      "Valid Loss = 0.5339531193004813\n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/20] Loss_D: 0.1919 Acc_D: 0.8284 Loss_G_class1: 0.0096 Loss_G_class2: 0.0121: 100%|██████████| 77/77 [01:50<00:00,  1.43s/it]\n",
      "[Validating]: Acc_D: 0.7391: 100%|██████████| 69/69 [00:01<00:00, 37.45it/s]\n",
      "  0%|          | 0/69 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Acc = 0.7391304347826086\n",
      "Valid Loss = 0.6657811182591579\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping, epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating]: Acc_D: 0.7101: 100%|██████████| 69/69 [00:01<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Summary of training for LAMBDA = 0.05 (best model values)\n",
      "Valid Acc = 0.7101449275362319\n",
      "Valid Loss = 0.5732312286272645\n"
     ]
    }
   ],
   "source": [
    "cit_model = cit_builder()\n",
    "cit_model.train(train_data, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "\n",
    "\n",
    "def sample_loader(sample):\n",
    "    s = ToTensor()(x).float()\n",
    "    s = Variable(s, requires_grad=False)\n",
    "    s = s.unsqueeze(0)  \n",
    "    return s\n",
    "\n",
    "self = cit_model\n",
    "\n",
    "for class_name in self._class_names:\n",
    "    self._G_dict[class_name]= self._G_dict[class_name].to(self._device)\n",
    "\n",
    "\n",
    "sample = test_data[40]\n",
    "label = lb1.inverse_transform(test_label[0])[0]\n",
    "x = ToTensor()(sample).float().unsqueeze(0).to(device)\n",
    "class_name = 'P'\n",
    "y = self._G_dict[class_name](x)\n",
    "y = y[0].cpu().detach().numpy()\n",
    "print(y.shape)\n",
    "y = np.moveaxis(y, 0, -1)\n",
    "#y = cv2.resize(y, dsize=(224, 224))\n",
    "plt.imshow(y)\n",
    "plt.imshow(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIT Classifier Results:\n",
      "Loss: 0.5907702616454042\n",
      "Acc: 0.7426900584795322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91111   0.69492   0.78846       118\n",
      "           1    0.55556   0.84906   0.67164        53\n",
      "\n",
      "    accuracy                        0.74269       171\n",
      "   macro avg    0.73333   0.77199   0.73005       171\n",
      "weighted avg    0.80091   0.74269   0.75225       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = cit_model.evaluate(test_data, test_label)\n",
    "print(\"CIT Classifier Results:\")\n",
    "print(\"Loss: {}\".format(metrics[0]))\n",
    "print(\"Acc: {}\".format(metrics[1]))\n",
    "print(metrics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_train_data, t_train_label = cit_model.transform_data(train_data, train_label, lb1, lb2)\n",
    "#t_test_data, t_test_label = cit_model.transform_data(test_data, test_label, lb1, lb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom torchvision.transforms import ToPILImage\\n\\nsave_path = \"../data/prueba-transformada/\"\\n\\n#image = t_train_data[0]\\n\\n#r = np.max(image) - np.min(image)\\n\\n#image = (image - np.min(image))/r\\n\\n#label = lb2.inverse_transform(t_train_label[0])[0]\\n#path = save_path + str(label) + \".png\"\\n#print(path)\\n#plt.imshow(image)\\n\\n#plt.imshow(t_train_data[0])\\n\\nnew_t_train_data = copy.deepcopy(t_train_data)\\n\\nfor i in range(len(new_t_train_data)):\\n    image = new_t_train_data[0]\\n    new_t_train_data[i] = np.asarray(ToPILImage()(image))\\n\\nprint(new_t_train_data[0])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "save_path = \"../data/prueba-transformada/\"\n",
    "\n",
    "#image = t_train_data[0]\n",
    "\n",
    "#r = np.max(image) - np.min(image)\n",
    "\n",
    "#image = (image - np.min(image))/r\n",
    "\n",
    "#label = lb2.inverse_transform(t_train_label[0])[0]\n",
    "#path = save_path + str(label) + \".png\"\n",
    "#print(path)\n",
    "#plt.imshow(image)\n",
    "\n",
    "#plt.imshow(t_train_data[0])\n",
    "\n",
    "new_t_train_data = copy.deepcopy(t_train_data)\n",
    "\n",
    "for i in range(len(new_t_train_data)):\n",
    "    image = new_t_train_data[0]\n",
    "    new_t_train_data[i] = np.asarray(ToPILImage()(image))\n",
    "\n",
    "print(new_t_train_data[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PTP': 3, 'PTN': 2, 'NTP': 1, 'NTN': 0, 'P': 1, 'N': 0}\n",
      "WARNING:tensorflow:From /mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/20\n",
      "153/153 [==============================] - 34s 221ms/step - loss: 0.7332 - categorical_accuracy: 0.6445 - val_loss: 0.6343 - val_categorical_accuracy: 0.6618\n",
      "Epoch 2/20\n",
      "153/153 [==============================] - 12s 77ms/step - loss: 0.4634 - categorical_accuracy: 0.8079 - val_loss: 0.6887 - val_categorical_accuracy: 0.6618\n",
      "Epoch 3/20\n",
      "153/153 [==============================] - 12s 77ms/step - loss: 0.2487 - categorical_accuracy: 0.9105 - val_loss: 0.8559 - val_categorical_accuracy: 0.6985\n",
      "Epoch 4/20\n",
      "153/153 [==============================] - 12s 79ms/step - loss: 0.1468 - categorical_accuracy: 0.9540 - val_loss: 0.7449 - val_categorical_accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "153/153 [==============================] - 12s 78ms/step - loss: 0.1043 - categorical_accuracy: 0.9655 - val_loss: 0.8917 - val_categorical_accuracy: 0.7279\n",
      "Epoch 6/20\n",
      "153/153 [==============================] - 12s 78ms/step - loss: 0.0973 - categorical_accuracy: 0.9713 - val_loss: 0.9220 - val_categorical_accuracy: 0.6985\n",
      "Epoch 7/20\n",
      "153/153 [==============================] - 12s 77ms/step - loss: 0.1017 - categorical_accuracy: 0.9598 - val_loss: 0.9269 - val_categorical_accuracy: 0.6985\n",
      "Epoch 8/20\n",
      "153/153 [==============================] - 12s 78ms/step - loss: 0.1091 - categorical_accuracy: 0.9631 - val_loss: 0.9628 - val_categorical_accuracy: 0.6912\n",
      "Epoch 9/20\n",
      "153/153 [==============================] - 12s 77ms/step - loss: 0.1218 - categorical_accuracy: 0.9573 - val_loss: 0.8351 - val_categorical_accuracy: 0.7647\n",
      "Epoch 10/20\n",
      "153/153 [==============================] - 12s 76ms/step - loss: 0.0400 - categorical_accuracy: 0.9894 - val_loss: 0.7804 - val_categorical_accuracy: 0.7426\n",
      "Epoch 11/20\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.0423 - categorical_accuracy: 0.9867Restoring model weights from the end of the best epoch.\n",
      "153/153 [==============================] - 13s 87ms/step - loss: 0.0421 - categorical_accuracy: 0.9868 - val_loss: 0.8987 - val_categorical_accuracy: 0.7574\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "dict_labels = { 'PTP' : np.argmax(lb2.transform(['PTP'])[0]) , 'PTN' : np.argmax(lb2.transform(['PTN'])[0]) , \n",
    "                'NTP' : np.argmax(lb2.transform(['NTP'])[0]) , 'NTN' : np.argmax(lb2.transform(['NTN'])[0]), \n",
    "                'P' : lb1.transform(['P'])[0][0], 'N' : lb1.transform(['N'])[0][0]\n",
    "              } \n",
    "\n",
    "print(dict_labels)\n",
    "\n",
    "from ClassifierModel import ClassifierModel\n",
    "\n",
    "classifier_model = classifier_builder(cit_model._G_dict)\n",
    "classifier_model.train(t_train_data, t_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDNET Classifier Results:\n",
      "Acc: 0.5263157894736842\n",
      "Acc_4: 0.672514619883041\n",
      "No concuerda: 58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.77647   0.73333   0.75429        90\n",
      "           1    0.72093   0.76543   0.74251        81\n",
      "\n",
      "    accuracy                        0.74854       171\n",
      "   macro avg    0.74870   0.74938   0.74840       171\n",
      "weighted avg    0.75016   0.74854   0.74871       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = classifier_model.evaluate(test_data, test_label)\n",
    "print(\"SDNET Classifier Results:\")\n",
    "print(\"Acc: {}\".format(metrics[0]))\n",
    "print(\"Acc_4: {}\".format(metrics[1]))\n",
    "print(\"No concuerda: {}\".format(metrics[2]))\n",
    "print(metrics[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds\n",
      "['P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P']\n",
      "preds_4\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3]\n",
      "Accuracy 4 clases: 0.23684210526315788\n",
      "Numero de veces no concuerda: 171\n",
      "{'N': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 90}, 'P': {'precision': 0.47368421052631576, 'recall': 1.0, 'f1-score': 0.6428571428571429, 'support': 81}, 'accuracy': 0.47368421052631576, 'macro avg': {'precision': 0.23684210526315788, 'recall': 0.5, 'f1-score': 0.32142857142857145, 'support': 171}, 'weighted avg': {'precision': 0.22437673130193903, 'recall': 0.47368421052631576, 'f1-score': 0.30451127819548873, 'support': 171}}\n",
      "0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdd/fcastro/envs/TF/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "G_dict = cit_model._G_dict\n",
    "dict_labels = { 'PTP' : np.argmax(lb2.transform(['PTP'])[0]) , 'PTN' : np.argmax(lb2.transform(['PTN'])[0]) , \n",
    "                    'NTP' : np.argmax(lb2.transform(['NTP'])[0]) , 'NTN' : np.argmax(lb2.transform(['NTN'])[0]) \n",
    "                } \n",
    "for key, _ in G_dict.items():\n",
    "    G_dict[key].to(\"cpu\")\n",
    "classifier_model.get_classification_report(test_files, dict_labels, G_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
